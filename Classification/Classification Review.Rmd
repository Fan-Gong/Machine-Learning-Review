---
title: "Classification Review"
author: "Fan Gong"
date: "2017/7/5"
output: github_document
---

#Classification 

##Preview
In terms of classification problems, we want solve the quesiton of estimating $Pr(Y=k|X=x_0)$. 

* For logistic regression, we assign this probability to logistic function. The *linear* decision boundary is easy to find when $P = 0.5$.

* For Linear Discriminant Analysis, we use bayes theorem to make $Pr(Y=k|X=x_0)=\frac{\pi_kf_k(x_0)}{\sum_{l=1}^{K}\pi_lf_l(x_0)}$ and assume $f_k(x_0)$ belongs to normal distribution. So we approximates the Bayes classifier by plugging estimates for $\pi_k$, $\mu_k$ and $\sigma^2$ into the above equation. The *linear* decision boundary is easy to find when $Pr(Y=1|X=x_0)=Pr(Y=0|X=x_0)$.

* For Quadratic Discriminant Analysis, we also assume it belongs to normal distribution but it allows each class has its own cavariance matrix.

* For the k-nearest neighbors algorithm, it is a non-parametric method.


##Logistic Regression

Logistic Regression involves directly modeling $Pr(Y=k|X=x)$ using the logistic function. 

* The logistic function is:

$p(x) = Pr(Y=1|X)=\frac{e^{\beta_0+\beta_1X}}{1+e^{\beta0+\beta1X}}$

* Log-odds or logit: 

$log(\frac{p(x)}{1-p(x)})=\beta_0+\beta_1X$ 

If the log-odds larger than zero, we assign it to class one. If the log-odds smaller than zero, we assign it to class two.
That is the reason we deem logistic regression as linear classifier. 

For *multiple logistic regression*, $log(\frac{p(x)}{1-p(x)})=\beta_0+\beta_1X_1+\cdots+\beta_pX_p$

The multiple-class logistic regression models are rarely used since discriminant analysis is more popular. Usually people will use two-class logistic regression. 

```{r}
library(ISLR)
attach(Smarket)
head(Smarket)

#data pre-processing
train = Smarket[Smarket$Year<2005,]
test = Smarket[Smarket$Year == 2005,]

glm.fit = glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, family = binomial, data = train)
summary(glm.fit)

#Make a prediction by using test data
glm.probs = predict(glm.fit, test, type = "response")
glm.pred = rep("Down", nrow(test))
glm.pred[glm.probs > 0.5] = "Up"

#test error rate
mean(glm.pred != test$Direction)

#The output is disappointing, we could only use lag1 to try again since it has the lowest p-value.

glm.fit2 = glm(Direction ~ Lag1 + Lag2, family = binomial, data = train)
summary(glm.fit2)
glm.probs2 = predict(glm.fit2, test, type = "response")
glm.pred2 = rep("Down", nrow(test))
glm.pred2[glm.probs2 > 0.5] = "Up"
mean(glm.pred2 != test$Direction)
```

##Linear Discriminant Analysis

###Comparison with logistic regression
* When the classes are well-separated, the parameter estimates for the logistic regression model are surprisingly unstable.
* If n is small and the distribution of the predictors X is approximately normal in each of the classes, the LDA model more stable.
* When we have more than two response classes, LDA is more popular. 

###Using Bayes' Theorem

To solve the quesiton of estimating $Pr(Y=k|X=x_0)$, the Bayes' theorem states that

$Pr(Y=k|X=x_0)=\frac{\pi_kf_k(x_0)}{\sum_{l=1}^{K}\pi_lf_l(x_0)}$. Here $\pi_k = p(Y=k)= \frac{\{\#i, y_i=k\}}{n}$;  $f_k(x)=p(X=x_0|Y=k)$ is the thing we want to estimate. Since if we have this estimation ,we could get $Pr(Y=k|X=x_0)$. To achieve this goal, we need to assume some simple forms for these densities.

For LDA, we will assume $f_k(x)$ belongs to multivariate normal distribution.
```{r}
#we still use the Smarket data
library(MASS)

lda.fit = lda(Direction ~ Lag1 + Lag2, data = train)

#The Coefficients of linear discriminants provides the linear combination of Lag1 and Lag2. If -0.642*Lag1-0.514*Lag2 is large, then the LDA classifier will predict a incease, and vice versa.
lda.fit

#Make a prediction
##The first element is class. Second is a matrix whose kth column contains the posterior probability that the corresponding observation belongs to the kth class. x contains the linear discriminants.
lda.pred = predict(lda.fit, test)
lda.class = lda.pred$class
table(lda.class, test$Direction)

#Test error
mean(lda.class == test$Direction)

#If we want to use a posterior probability threshold other than 50% in order to make a predictions, we could do like that:
sum(lda.pred$posterior[,1] > 0.51)

```

##Quadratic Discriminant Analysis

###Comparison with LDA

* LDA assumes that the observations within each class are drawn from a multivariate Gaussian distribution with a class-specific mean vector and a convariance matrix that is common to all k classes. QDA also assume normal dis but it allows each class has its own cavariance matrix.

* LDA is a much less flexible classifier than QDA(has fewer parameters), and so has lower variance. So, LDA tends to be a better bet than QDA if there are few observations and reducing variance is crucial. In contrast, QDA is recommended if the training set is very large, so that the variance is not a major concern.

```{r}
#syntax is identical to lda
qda.fit = qda(Direction ~ Lag1 + Lag2, data = train)
qda.fit

#make a prediction
qda.class = predict(qda.fit, test)$class
mean(qda.class == test$Direction)
```

##K-Nearest Neighbors

`knn()` function requires four inputs
1. A matrix containing the predictors associated with the training data.
2. A matrix containing the predictors associated with the data for which we wish to make predictions
3. A vector containing the class labels for the training observations
4. A value for K, the number of nearest neighbors to be used by the classifier.
```{r}
library(class)
#data pre-processing
train.X = data.frame(Lag1 = train$Lag1, Lag2 = train$Lag2)
test.X = data.frame(Lag1 = test$Lag1, Lag2 = test$Lag2)
train.Direction = train$Direction

#make a prediction
set.seed(1)
knn.pred = knn(train.X, test.X, train.Direction, k = 1)

mean(knn.pred == test$Direction)

#try differenent K
knn.pred2 = knn(train.X, test.X, train.Direction, k = 3)
mean(knn.pred2 == test$Direction)
```

##A Comparison of Classification Methods

* LDA and Logistic regression are quite similar, they differ only in their fitting procedures.

* KNN is a non-parametric approach, it performs better when the decision boundary is highly non-linear.

* QDA serves as a compromise between the KNN and LDA/logistic


